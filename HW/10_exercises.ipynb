{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural & Behavioral Modeling - Week 10 (Exercises)\n",
    "by Ting-Yu, Lin (tylin.amice@gmail.om)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True \n",
    "%matplotlib inline\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "from IPython.display import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 A Two-layered Linear Network as a Regression Model (7 points)\n",
    "Data fitting of the following network is poor. Please check if adding bias terms or chaging network hyperparameters (e.g., learning rate, amount of training, etc.) help. If not, please explain why the fitting is poor given that the network/regression model has sufficient degrees of freedom (i.e., network weights or regression coefficients) to overfit such a small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal results:\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Reality:\n",
      "[[ 0.679 -0.   ]\n",
      " [ 0.643  0.   ]\n",
      " [ 1.321  0.   ]\n",
      " [ 0.     0.643]\n",
      " [-0.     0.643]\n",
      " [ 0.     1.286]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5v8AXCzJSM1SkekAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104f3c2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we train a two-layered network of units \n",
    "# with a linear activation function f(x)=x\n",
    "# to associate patterns using the delta rule dW=(t-y)*x\n",
    "\n",
    "set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "X=array([[1,0,0,0],[0,1,0,0],[1,1,0,0],[0,0,1,0],[0,0,0,1],[0,0,1,1]])\n",
    "Y=array([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1]])\n",
    "[Np,Nx]=X.shape; # find numbers of patterns and input dimensions\n",
    "[Np,Ny]=Y.shape; # find numbers of patterns and output dimensions\n",
    "W=random.rand(Ny,Nx); # set initially random connectivity matrix\n",
    "\n",
    "eta=.1; # set the learning rate \n",
    "tol=1e-2; # set the tolerance/stopping criterion; try 0.01\n",
    "nIts=50000; # set the maximum number of allowed iterations\n",
    "totErr=10; # set the maximum training error to an initially high value\n",
    "totErr_hist=[] # history of totall error\n",
    "\n",
    "for c in range(nIts): # for each learning iteration\n",
    "    p=mod(c,Np) # sequential presentation of the training samples\n",
    "    #p=random.randint(nP); # choose a traing pattern at random\n",
    "    \n",
    "    # Forward propagation:\n",
    "    y=W.dot(X[p])\n",
    "    \n",
    "    # Backward propagation:\n",
    "    deltaW=eta*outer(Y[p].T-y,X[p]) # delta learning\n",
    "    W=W+deltaW;  # apply the weight update\n",
    "    \n",
    "    # Checking if done:\n",
    "    if(mod(c,10*Np)==0): # after 10 updates check total errors\n",
    "        predY=W.dot(X.T) # testing ALL the training samples\n",
    "        totErr=sum((Y.T-predY)**2) # sum of squared errors for all samples\n",
    "        totErr_hist.append(totErr)\n",
    "    if(totErr<tol):\n",
    "        break # break if max error is below tolerance\n",
    "        \n",
    "plot(totErr_hist);\n",
    "title(str(c)+':'+str(totErr));\n",
    "print('Ideal results:')\n",
    "print(Y)\n",
    "print('Reality:')\n",
    "print(predY.T) # predicted Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write your discussions here, if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 PyTorch (3 points)\n",
    "Read <a href=\"http://noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94Numpy%EF%BC%8CTorch%E5%AF%B9%E6%AF%94/\">this tutorial</a> first and port the following Instar Learning from NumPy to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30950744  0.37140632  0.22655833] 0.255651802926\n",
      "[ 0.25594649  0.35315116  0.29646418] 0.27977208961\n",
      "[ 0.21231701  0.33828095  0.35340782] 0.299419898849\n",
      "[ 0.17868706  0.32681887  0.39730044] 0.31456458762\n",
      "[ 0.1539349   0.31838261  0.42960608] 0.325711313502\n",
      "[ 0.13636769  0.31239518  0.45253418] 0.333622413485\n",
      "[ 0.12423462  0.30825987  0.46836984] 0.339086343425\n",
      "[ 0.11601699  0.30545906  0.4790952 ] 0.342787015326\n",
      "[ 0.11052657  0.30358777  0.48626109] 0.345259532814\n",
      "[ 0.10689217  0.30234906  0.49100458] 0.3468962243\n"
     ]
    }
   ],
   "source": [
    "# Instar learning:\n",
    "x=array([0.1,0.3,0.5])\n",
    "W1=random.rand(3); W2=random.rand(3)\n",
    "y=dot(W1,x) # Bottom-up F1→F2\n",
    "for i in range(10): # trials \n",
    "    y=dot(W1,x) # Bottom-up F1→F2\n",
    "    print(W1,y)\n",
    "    W1+=y*(x-W1) # Postsynaptically gated InStar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your PyTorch codes here\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2736,  0.6962,  0.3444]) tensor(0.4084)\n",
      "tensor([ 0.2027,  0.5344,  0.4079]) tensor(0.3846)\n",
      "tensor([ 0.1632,  0.4442,  0.4433]) tensor(0.3713)\n",
      "tensor([ 0.1397,  0.3907,  0.4644]) tensor(0.3634)\n",
      "tensor([ 0.1253,  0.3577,  0.4773]) tensor(0.3585)\n",
      "tensor([ 0.1162,  0.3370,  0.4855]) tensor(0.3555)\n",
      "tensor([ 0.1105,  0.3239,  0.4906]) tensor(0.3535)\n",
      "tensor([ 0.1068,  0.3154,  0.4939]) tensor(0.3523)\n",
      "tensor([ 0.1044,  0.3100,  0.4961]) tensor(0.3515)\n",
      "tensor([ 0.1028,  0.3065,  0.4975]) tensor(0.3510)\n"
     ]
    }
   ],
   "source": [
    "t_x = t.tensor([0.1,0.3,0.5])\n",
    "t_W1 = t.rand(3); t_W2 = t.rand(3)\n",
    "t_y = (t_W1*t_x).sum() #t.dot跑出來很怪\n",
    "for i in range(10):\n",
    "    t_y = (t_W1*t_x).sum()\n",
    "    print(t_W1,t_y)\n",
    "    t_W1+=t_y*(t_x-t_W1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
