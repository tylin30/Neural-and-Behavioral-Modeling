{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural & Behavioral Modeling - Week 10 (Exercises)\n",
    "by Ting-Yu, Lin (tylin.amice@gmail.om)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True \n",
    "%matplotlib inline\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "from IPython.display import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 A Two-layered Linear Network as a Regression Model (7 points)\n",
    "Data fitting of the following network is poor. Please check if adding bias terms or chaging network hyperparameters (e.g., learning rate, amount of training, etc.) help. If not, please explain why the fitting is poor given that the network/regression model has sufficient degrees of freedom (i.e., network weights or regression coefficients) to overfit such a small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal results:\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Reality:\n",
      "[[ 0.679 -0.   ]\n",
      " [ 0.643  0.   ]\n",
      " [ 1.321  0.   ]\n",
      " [-0.     0.643]\n",
      " [ 0.     0.643]\n",
      " [ 0.     1.286]]\n",
      "W: [[ 0.679  0.679 -0.     0.   ]\n",
      " [-0.     0.     0.643  0.643]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGNySNGEMbkmaMAa3JE2Y/wMBfJJH1meSmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d756e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we train a two-layered network of units \n",
    "# with a linear activation function f(x)=x\n",
    "# to associate patterns using the delta rule dW=(t-y)*x\n",
    "\n",
    "set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "X=array([[1,0,0,0],[0,1,0,0],[1,1,0,0],[0,0,1,0],[0,0,0,1],[0,0,1,1]])\n",
    "#X=array([[1,0,0,0,1],[0,1,0,0,1],[1,1,0,0,1],[0,0,1,0,1],[0,0,0,1,1],[0,0,1,1,1]]) #bias\n",
    "#X=array([[1,0,0,0],[1,0,0,0],[1,0,0,0],[1,0,0,0],[1,0,0,0],[1,0,0,0]])\n",
    "Y=array([[1,0],[1,0],[1,0],[0,1],[0,1],[0,1]])\n",
    "[Np,Nx]=X.shape; # find numbers of patterns and input dimensions\n",
    "[Np,Ny]=Y.shape; # find numbers of patterns and output dimensions\n",
    "W=random.rand(Ny,Nx); # set initially random connectivity matrix\n",
    "\n",
    "eta=.1; # set the learning rate \n",
    "tol=1e-2; # set the tolerance/stopping criterion; try 0.01\n",
    "nIts=50000; # set the maximum number of allowed iterations\n",
    "totErr=10; # set the maximum training error to an initially high value\n",
    "totErr_hist=[] # history of totall error\n",
    "\n",
    "for c in range(nIts): # for each learning iteration\n",
    "    p=mod(c,Np) # sequential presentation of the training samples\n",
    "    #p=random.randint(Np); # choose a traing pattern at random\n",
    "    \n",
    "    # Forward propagation:\n",
    "    y=W.dot(X[p])\n",
    "    \n",
    "    # Backward propagation:\n",
    "    deltaW=eta*outer(Y[p].T-y,X[p]) # delta learning\n",
    "    W=W+deltaW;  # apply the weight update\n",
    "    \n",
    "    # Checking if done:\n",
    "    if(mod(c,10*Np)==0): # after 10 updates check total errors\n",
    "        predY=W.dot(X.T) # testing ALL the training samples\n",
    "        totErr=sum((Y.T-predY)**2) # sum of squared errors for all samples\n",
    "        totErr_hist.append(totErr)\n",
    "    if(totErr<tol):\n",
    "        print(\"break\")\n",
    "        break # break if max error is below tolerance\n",
    "        \n",
    "plot(totErr_hist);\n",
    "title(str(c)+':'+str(totErr));\n",
    "print('Ideal results:')\n",
    "print(Y)\n",
    "print('Reality:')\n",
    "print(predY.T) # predicted Y\n",
    "print(\"W:\",W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write your discussions here, if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "無論是加入bias或是調整各種parameters都沒有辦法在在許可的錯誤範圍內達到自己的預期.\n",
    "\n",
    "進一步看data, 會發現其實以x的前兩項為一組和後兩項為一組, 分別做OR Gate即能達到perfectly match的結果. 且data性質看起來應該是類別型資料.\n",
    "\n",
    "以目前Two-layered Linear Network的架構下要成功達到想要的結果, 即以相加的方式($y_j=\\sum_{i=1}^{n} (x_i*w_ij)\n",
    "$)得到可接受的Predicted$y_j$幾乎是不可行(無法達到OR Gate的效果). 故這種情況一種可行的解法是改變架構, 在$y_j$上放一個activation function, 挑選一個合適的function讓結果能極端地跑出1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 PyTorch (3 points)\n",
    "Read <a href=\"http://noahsnail.com/2017/09/18/2017-9-18-PyTorch%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95(%E4%B8%80)%E2%80%94%E2%80%94Numpy%EF%BC%8CTorch%E5%AF%B9%E6%AF%94/\">this tutorial</a> first and port the following Instar Learning from NumPy to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.694  0.054  0.342] 0.256651050133\n",
      "[ 0.542  0.117  0.383] 0.280609156145\n",
      "[ 0.418  0.169  0.415] 0.300080862284\n",
      "[ 0.322  0.208  0.441] 0.315060640174\n",
      "[ 0.252  0.237  0.459] 0.326068657248\n",
      "[ 0.203  0.258  0.473] 0.333871918045\n",
      "[ 0.168  0.272  0.482] 0.339256631702\n",
      "[ 0.145  0.281  0.488] 0.342901390644\n",
      "[ 0.13   0.288  0.492] 0.345335513664\n",
      "[ 0.119  0.292  0.495] 0.346946326449\n"
     ]
    }
   ],
   "source": [
    "# Instar learning:\n",
    "x=array([0.1,0.3,0.5])\n",
    "W1=random.rand(3); W2=random.rand(3)\n",
    "y=dot(W1,x) # Bottom-up F1→F2\n",
    "for i in range(10): # trials \n",
    "    y=dot(W1,x) # Bottom-up F1→F2\n",
    "    print(W1,y)\n",
    "    W1+=y*(x-W1) # Postsynaptically gated InStar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your PyTorch codes here\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n",
      "tensor([ 0.4831,  0.8740,  0.0945]) tensor(1.00000e-19 *\n",
      "       -1.0842)\n"
     ]
    }
   ],
   "source": [
    "t_x = t.tensor([0.1,0.3,0.5])\n",
    "t_W1 = t.rand(3); t_W2 = t.rand(3)\n",
    "t_y = t.dot(t_W1, t_x)\n",
    "#t_y = (t_W1*t_x).sum()\n",
    "for i in range(10):\n",
    "    #t_y = (t_W1*t_x).sum()\n",
    "    t_y = t.dot(t_W1,t_x)\n",
    "    print(t_W1,t_y)\n",
    "    t_W1+=t_y*(t_x-t_W1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
