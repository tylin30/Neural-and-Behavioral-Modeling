{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural & Behavioral Modeling - Week 12 (Exercises)\n",
    "by Ting-Yu, Lin (tylin.amice@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade PyTorch to 0.4.0 if necessary:\n",
    "! conda install -y pytorch-cpu torchvision-cpu -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True \n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import *\n",
    "from IPython.display import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 0.4.0\n",
      "Device  0 : Tesla K80\n",
      "Device  1 : Tesla K80\n",
      "Current: Device  0\n"
     ]
    }
   ],
   "source": [
    "# Check GPU status:\n",
    "import torch as t\n",
    "print('PyTorch version:',t.__version__)\n",
    "use_cuda=t.cuda.is_available()\n",
    "if(use_cuda):\n",
    "    for i in range(t.cuda.device_count()):\n",
    "        print('Device ',i,':',t.cuda.get_device_name(i))\n",
    "    print('Current: Device ',t.cuda.current_device())\n",
    "    t.backends.cudnn.benchmark = True \n",
    "    device = t.device(\"cuda\")\n",
    "else:\n",
    "    device = t.device(\"cpu\")\n",
    "    print('No GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 18 09:47:49 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    56W / 149W |   2875MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K80           Off  | 00000000:05:00.0 Off |                    0 |\r\n",
      "| N/A   32C    P8    31W / 149W |     37MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     19920      C   /raid/opt/hpc/anaconda/python3/bin/python    651MiB |\r\n",
      "|    0     22788      C   /raid/opt/hpc/anaconda/python3/bin/python    906MiB |\r\n",
      "|    0     23242      C   /raid/opt/hpc/anaconda/python3/bin/python    957MiB |\r\n",
      "|    0     34422      C   /raid/opt/hpc/anaconda/python3/bin/python    321MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Fair Performance Assessment (5 points)\n",
    "We often compare and assess performances of different model architectures/parameters/hyperparameters. Note that the results are differnt even if you re-run exactly the same code block. This is primarily due to a non-fixed random number seed. Please:\n",
    "\n",
    "(1) run the section 1.2 TEN times and report (a) min, (b) max, (c) mean, & (d) standard deviation of the TESTING accuracies. (3 points)\n",
    "\n",
    "(2) try to fix the random number seeds in numpy & pytorch to see if you can obtain the same results every time in the section 1.2. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset:\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "train_set = CIFAR10(root='.', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_data = t.utils.data.DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "test_set = CIFAR10(root='.', train=False, transform=transforms.ToTensor())\n",
    "test_data = t.utils.data.DataLoader(train_set, batch_size=1000, shuffle=False)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model:\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # = nn.Module.__init__(self)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # in, out, kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) \n",
    "        self.fc1   = nn.Linear(16*5*5, 120) \n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "    def forward(self, x): # functional expressions\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) \n",
    "        x = x.view(x.size()[0], -1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)        \n",
    "        return x\n",
    "lenet = Net()\n",
    "lenet = lenet.to(device)\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(lenet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 : 0.3125\n",
      "epoch  1 : 0.3125\n",
      "test : 0.509\n",
      "epoch  0 : 0.375\n",
      "epoch  1 : 0.375\n",
      "test : 0.562\n",
      "epoch  0 : 0.375\n",
      "epoch  1 : 0.375\n",
      "test : 0.608\n",
      "epoch  0 : 0.375\n",
      "epoch  1 : 0.5625\n",
      "test : 0.629\n",
      "epoch  0 : 0.4375\n",
      "epoch  1 : 0.5625\n",
      "test : 0.64\n",
      "epoch  0 : 0.5625\n",
      "epoch  1 : 0.75\n",
      "test : 0.663\n",
      "epoch  0 : 0.75\n",
      "epoch  1 : 0.75\n",
      "test : 0.686\n",
      "epoch  0 : 0.75\n",
      "epoch  1 : 0.8125\n",
      "test : 0.693\n",
      "epoch  0 : 0.8125\n",
      "epoch  1 : 0.75\n",
      "test : 0.702\n",
      "epoch  0 : 0.8125\n",
      "epoch  1 : 0.75\n",
      "test : 0.701\n",
      "[0.509 0.562 0.608 0.629 0.64  0.663 0.686 0.693 0.702 0.701]\n"
     ]
    }
   ],
   "source": [
    "#test N times (N=10)\n",
    "Ntest = 10\n",
    "testAccArr = np.empty(Ntest)\n",
    "for j in range(Ntest):\n",
    "    # Training:\n",
    "    for e in range(2):\n",
    "        for i, (X_train, Y_train) in enumerate(train_data, 0):\n",
    "            X_train,Y_train=X_train.to(device),Y_train.to(device)\n",
    "            Y_pred = lenet(X_train)\n",
    "            loss = loss_fn(Y_pred, Y_train)\n",
    "            lenet.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            Y_pred = lenet(X_train)\n",
    "            Y_pred = t.max(Y_pred,1)[1]\n",
    "        print('epoch ',e,':',(Y_pred==Y_train).sum().item()/Y_train.shape[0])\n",
    "        \n",
    "    # Testing on a batch:\n",
    "    dataiter = iter(test_data)\n",
    "    X_test, Y_test = dataiter.next() # returning a batch\n",
    "    X_test,Y_test=X_test.to(device),Y_test.to(device)\n",
    "    with t.no_grad():\n",
    "        Y_pred = lenet(X_test)\n",
    "        Y_pred = t.max(Y_pred,1)[1]\n",
    "        print('test :',(Y_pred==Y_test).sum().item()/Y_test.shape[0])\n",
    "        testAccArr[j] = (Y_pred==Y_test).sum().item()/Y_test.shape[0]\n",
    "print(testAccArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW11 1.1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  0.509 max:  0.702 mean:  0.6393 sd: 0.06115889142226172\n"
     ]
    }
   ],
   "source": [
    "testmin = np.min(testAccArr)\n",
    "testmax = np.max(testAccArr)\n",
    "testmean = np.mean(testAccArr)\n",
    "testsd = np.std(testAccArr)\n",
    "print(\"min: \",testmin,\"max: \",testmax, \"mean: \",testmean, \"sd:\", testsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "epoch  0 : 0.9375\n",
      "epoch  1 : 0.9375\n",
      "test : 0.756\n",
      "1\n",
      "epoch  0 : 0.9375\n",
      "epoch  1 : 1.0\n",
      "test : 0.727\n",
      "2\n",
      "epoch  0 : 0.9375\n",
      "epoch  1 : 1.0\n",
      "test : 0.735\n",
      "3\n",
      "epoch  0 : 1.0\n",
      "epoch  1 : 1.0\n",
      "test : 0.765\n",
      "4\n",
      "epoch  0 : 1.0\n",
      "epoch  1 : 1.0\n",
      "test : 0.754\n",
      "5\n",
      "epoch  0 : 1.0\n",
      "epoch  1 : 1.0\n",
      "test : 0.75\n",
      "6\n",
      "epoch  0 : 0.9375\n",
      "epoch  1 : 1.0\n",
      "test : 0.721\n",
      "7\n",
      "epoch  0 : 0.9375\n",
      "epoch  1 : 0.875\n",
      "test : 0.749\n",
      "8\n",
      "epoch  0 : 0.9375\n",
      "epoch  1 : 1.0\n",
      "test : 0.753\n",
      "9\n",
      "epoch  0 : 1.0\n",
      "epoch  1 : 1.0\n",
      "test : 0.75\n",
      "[0.756 0.727 0.735 0.765 0.754 0.75  0.721 0.749 0.753 0.75 ]\n"
     ]
    }
   ],
   "source": [
    "#set random seed\n",
    "#np.random.seed(1)\n",
    "#t.manual_seed(1)\n",
    "#t.cuda.manual_seed_all(1)\n",
    "#test random seed\n",
    "#np.random.rand()\n",
    "#t.randn(1)\n",
    "\n",
    "Ntest = 10\n",
    "testAccArr = np.empty(Ntest)\n",
    "\n",
    "\n",
    "for j in range(Ntest):\n",
    "    # Training:\n",
    "    #np.random.seed(j)\n",
    "    #t.manual_seed(j)\n",
    "    print(j)\n",
    "    for e in range(2):\n",
    "        #np.random.seed(j+e*100)\n",
    "        #t.manual_seed(j+e*100)\n",
    "        #print(j)\n",
    "        for i, (X_train, Y_train) in enumerate(train_data, 0):\n",
    "            X_train,Y_train=X_train.to(device),Y_train.to(device)\n",
    "            Y_pred = lenet(X_train)\n",
    "            loss = loss_fn(Y_pred, Y_train)\n",
    "            lenet.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            Y_pred = lenet(X_train)\n",
    "            Y_pred = t.max(Y_pred,1)[1]\n",
    "        print('epoch ',e,':',(Y_pred==Y_train).sum().item()/Y_train.shape[0])\n",
    "        \n",
    "    # Testing on a batch:\n",
    "    dataiter = iter(test_data)\n",
    "    X_test, Y_test = dataiter.next() # returning a batch\n",
    "    X_test,Y_test=X_test.to(device),Y_test.to(device)\n",
    "    with t.no_grad():\n",
    "        Y_pred = lenet(X_test)\n",
    "        Y_pred = t.max(Y_pred,1)[1]\n",
    "        print('test :',(Y_pred==Y_test).sum().item()/Y_test.shape[0])\n",
    "        testAccArr[j] = (Y_pred==Y_test).sum().item()/Y_test.shape[0]\n",
    "#np.random.seed(1)\n",
    "#t.manual_seed(1)\n",
    "\n",
    "print(testAccArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Your answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試過了用numpy, pytorch(GPU版,非GPU版都試過)設random seed，也用最簡單的random測試過random seed的code沒有問題，也有試著在不同的地方，如：一開頭、不同迴圈的頭設random seed，但依舊無法重製。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Universal Approximation Theorem (5 points)\n",
    "\n",
    "Please FAIRLY assess whether a deep network learns XOR more efficiently than a shallow network with the same number of model parameters. Please discuss why in either case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 XOR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neEpyXywAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aed78008438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N=1000 # samples per cluster\n",
    "XY=t.tensor([[5,5],[5,10],[10,5],[10,10]],dtype=t.float32) # 4 cluster centers\n",
    "Z=t.tensor([0,1,1,0]) # category labels\n",
    "t.cat([t.randn(2,1)+XY[0,0],t.randn(2,1)+XY[0,1]],1)\n",
    "xy,z=t.zeros(4*N,2),t.zeros(4*N,dtype=t.int64)\n",
    "for i in range(4):\n",
    "    xy[i*N:(i+1)*N,]=t.rand(N,2)+XY[i,]\n",
    "    z[i*N:(i+1)*N]=Z[i]\n",
    "xy_np=xy.numpy()\n",
    "z_np=z.numpy().astype(int)\n",
    "cmap=np.array([[1,0,0],[0,1,0]])\n",
    "scatter(xy_np[:,0],xy_np[:,1],color=cmap[z_np]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 A shallow net with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 : 0.26\n",
      "epoch  1 : 0.274\n",
      "epoch  2 : 0.289\n",
      "epoch  3 : 0.3135\n",
      "epoch  4 : 0.33575\n",
      "epoch  5 : 0.36325\n",
      "epoch  6 : 0.40475\n",
      "epoch  7 : 0.48325\n",
      "epoch  8 : 0.5\n",
      "epoch  9 : 0.5\n",
      "epoch  10 : 0.5\n",
      "epoch  11 : 0.5\n",
      "epoch  12 : 0.5015\n",
      "epoch  13 : 0.50875\n",
      "epoch  14 : 0.5305\n",
      "epoch  15 : 0.5705\n",
      "epoch  16 : 0.71475\n",
      "epoch  17 : 0.74025\n",
      "epoch  18 : 0.75\n",
      "epoch  19 : 0.75\n",
      "epoch  20 : 0.75\n",
      "epoch  21 : 0.75\n",
      "epoch  22 : 0.75\n",
      "epoch  23 : 0.75\n",
      "epoch  24 : 0.75\n",
      "epoch  25 : 0.75\n",
      "epoch  26 : 0.75\n",
      "epoch  27 : 0.75\n",
      "epoch  28 : 0.75\n",
      "epoch  29 : 0.75\n",
      "epoch  30 : 0.75\n",
      "epoch  31 : 0.75\n",
      "epoch  32 : 0.75\n",
      "epoch  33 : 0.75\n",
      "epoch  34 : 0.751\n",
      "epoch  35 : 0.8045\n",
      "epoch  36 : 0.89775\n",
      "epoch  37 : 0.922\n",
      "epoch  38 : 0.9455\n",
      "epoch  39 : 0.9575\n",
      "epoch  40 : 0.96425\n",
      "epoch  41 : 0.9725\n",
      "epoch  42 : 0.9775\n",
      "epoch  43 : 0.98225\n",
      "epoch  44 : 0.9865\n",
      "epoch  45 : 0.9905\n",
      "epoch  46 : 0.99425\n",
      "epoch  47 : 0.99625\n",
      "epoch  48 : 0.998\n",
      "epoch  49 : 0.99925\n",
      "epoch  50 : 1.0\n",
      "epoch  51 : 1.0\n",
      "epoch  52 : 1.0\n",
      "epoch  53 : 1.0\n",
      "epoch  54 : 1.0\n",
      "epoch  55 : 1.0\n",
      "epoch  56 : 1.0\n",
      "epoch  57 : 1.0\n",
      "epoch  58 : 1.0\n",
      "epoch  59 : 1.0\n",
      "epoch  60 : 1.0\n",
      "epoch  61 : 1.0\n",
      "epoch  62 : 1.0\n",
      "epoch  63 : 1.0\n",
      "epoch  64 : 1.0\n",
      "epoch  65 : 1.0\n",
      "epoch  66 : 1.0\n",
      "epoch  67 : 1.0\n",
      "epoch  68 : 1.0\n",
      "epoch  69 : 1.0\n",
      "epoch  70 : 1.0\n",
      "epoch  71 : 1.0\n",
      "epoch  72 : 1.0\n",
      "epoch  73 : 1.0\n",
      "epoch  74 : 1.0\n",
      "epoch  75 : 1.0\n",
      "epoch  76 : 1.0\n",
      "epoch  77 : 1.0\n",
      "epoch  78 : 1.0\n",
      "epoch  79 : 1.0\n",
      "epoch  80 : 1.0\n",
      "epoch  81 : 1.0\n",
      "epoch  82 : 1.0\n",
      "epoch  83 : 1.0\n",
      "epoch  84 : 1.0\n",
      "epoch  85 : 1.0\n",
      "epoch  86 : 1.0\n",
      "epoch  87 : 1.0\n",
      "epoch  88 : 1.0\n",
      "epoch  89 : 1.0\n",
      "epoch  90 : 1.0\n",
      "epoch  91 : 1.0\n",
      "epoch  92 : 1.0\n",
      "epoch  93 : 1.0\n",
      "epoch  94 : 1.0\n",
      "epoch  95 : 1.0\n",
      "epoch  96 : 1.0\n",
      "epoch  97 : 1.0\n",
      "epoch  98 : 1.0\n",
      "epoch  99 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Number of free parameters: 2*H+H*2=70\n",
    "\n",
    "# H=35 # number of hidden units\n",
    "H= 35\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.Linear(2, H, bias=False),\n",
    "    t.nn.BatchNorm1d(H),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(H, 2, bias=False),\n",
    "#     t.nn.Sigmoid()\n",
    "#     t.nn.Softmax(dim=1)\n",
    ")\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(model.parameters())\n",
    "\n",
    "for i in range(100):\n",
    "    z_pred = model(xy)\n",
    "    loss = loss_fn(z_pred,z)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    z_pred = model(xy) \n",
    "    z_pred = t.max(z_pred,1)[1]\n",
    "    print('epoch ',i,':',(z_pred==z).sum().item()/xy.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 A \"deep\" net with three hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 : 0.75\n",
      "epoch  1 : 0.75\n",
      "epoch  2 : 0.75\n",
      "epoch  3 : 0.75\n",
      "epoch  4 : 0.75\n",
      "epoch  5 : 0.75\n",
      "epoch  6 : 0.75\n",
      "epoch  7 : 0.75\n",
      "epoch  8 : 0.75\n",
      "epoch  9 : 0.75\n",
      "epoch  10 : 0.75\n",
      "epoch  11 : 0.75\n",
      "epoch  12 : 0.75\n",
      "epoch  13 : 0.75\n",
      "epoch  14 : 0.75\n",
      "epoch  15 : 0.75\n",
      "epoch  16 : 0.75\n",
      "epoch  17 : 0.75\n",
      "epoch  18 : 0.75\n",
      "epoch  19 : 0.75\n",
      "epoch  20 : 0.75\n",
      "epoch  21 : 0.75\n",
      "epoch  22 : 0.75\n",
      "epoch  23 : 0.75\n",
      "epoch  24 : 0.75\n",
      "epoch  25 : 0.75\n",
      "epoch  26 : 0.75\n",
      "epoch  27 : 0.75\n",
      "epoch  28 : 0.75\n",
      "epoch  29 : 0.75\n",
      "epoch  30 : 0.75\n",
      "epoch  31 : 0.75\n",
      "epoch  32 : 0.75\n",
      "epoch  33 : 0.75\n",
      "epoch  34 : 0.75\n",
      "epoch  35 : 0.75\n",
      "epoch  36 : 0.75\n",
      "epoch  37 : 0.75\n",
      "epoch  38 : 0.75\n",
      "epoch  39 : 0.75\n",
      "epoch  40 : 0.75\n",
      "epoch  41 : 0.75\n",
      "epoch  42 : 0.75\n",
      "epoch  43 : 0.75\n",
      "epoch  44 : 0.75\n",
      "epoch  45 : 0.75\n",
      "epoch  46 : 0.75\n",
      "epoch  47 : 0.75\n",
      "epoch  48 : 0.75\n",
      "epoch  49 : 0.75\n",
      "epoch  50 : 0.75\n",
      "epoch  51 : 0.75\n",
      "epoch  52 : 0.75\n",
      "epoch  53 : 0.75\n",
      "epoch  54 : 0.75\n",
      "epoch  55 : 0.75\n",
      "epoch  56 : 0.75\n",
      "epoch  57 : 0.75\n",
      "epoch  58 : 0.75\n",
      "epoch  59 : 0.75\n",
      "epoch  60 : 0.75\n",
      "epoch  61 : 0.75\n",
      "epoch  62 : 0.75\n",
      "epoch  63 : 0.75\n",
      "epoch  64 : 0.75\n",
      "epoch  65 : 0.75\n",
      "epoch  66 : 0.75\n",
      "epoch  67 : 0.75\n",
      "epoch  68 : 0.75\n",
      "epoch  69 : 0.99825\n",
      "epoch  70 : 0.99925\n",
      "epoch  71 : 0.99975\n",
      "epoch  72 : 0.99975\n",
      "epoch  73 : 1.0\n",
      "epoch  74 : 1.0\n",
      "epoch  75 : 1.0\n",
      "epoch  76 : 1.0\n",
      "epoch  77 : 1.0\n",
      "epoch  78 : 1.0\n",
      "epoch  79 : 1.0\n",
      "epoch  80 : 1.0\n",
      "epoch  81 : 1.0\n",
      "epoch  82 : 1.0\n",
      "epoch  83 : 1.0\n",
      "epoch  84 : 1.0\n",
      "epoch  85 : 1.0\n",
      "epoch  86 : 1.0\n",
      "epoch  87 : 1.0\n",
      "epoch  88 : 1.0\n",
      "epoch  89 : 1.0\n",
      "epoch  90 : 1.0\n",
      "epoch  91 : 1.0\n",
      "epoch  92 : 1.0\n",
      "epoch  93 : 1.0\n",
      "epoch  94 : 1.0\n",
      "epoch  95 : 1.0\n",
      "epoch  96 : 1.0\n",
      "epoch  97 : 1.0\n",
      "epoch  98 : 1.0\n",
      "epoch  99 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Number of free parameters: 2*H+H*H+H*H+H*2=70\n",
    "\n",
    "H=5 # number of hidden units\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.Linear(2, H, bias=False),\n",
    "    t.nn.BatchNorm1d(H),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(H, H,bias=False),\n",
    "    t.nn.BatchNorm1d(H),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(H, H, bias=False),\n",
    "    t.nn.BatchNorm1d(H),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(H, 2, bias=False),\n",
    "    t.nn.Softmax(dim=1)\n",
    ")\n",
    "loss_fn = t.nn.CrossEntropyLoss()\n",
    "optimizer = t.optim.Adam(model.parameters())\n",
    "\n",
    "for i in range(100):\n",
    "    z_pred = model(xy)\n",
    "    loss = loss_fn(z_pred,z)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    z_pred = model(xy) \n",
    "    z_pred = t.max(z_pred,1)[1]\n",
    "    print('epoch ',i,':',(z_pred==z).sum().item()/xy.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Your answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這個case中，看起來\"deep\"的表現不一定會比\"shallow\"來得好。(達到ACC = 1之epoch數沒有比較小, 甚至有時候會出現沒有收斂的狀態(一直達不到ACC = 1))\n",
    "\n",
    "原因可能是XOR算是一個簡單的問題，所以deep net能夠學習子類別、減小search space的優勢就沒有出來。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
